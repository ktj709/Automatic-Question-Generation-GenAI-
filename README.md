## ğŸ§  Document Intelligence System
## Multi-Modal PDF Question Answering with Reranking, MCQ Generation & Local Evaluation Suite

## ğŸš€ Overview

This project is an end-to-end Document Intelligence System built using Streamlit, integrating retrieval-augmented generation (RAG), cross-encoder reranking, multimodal extraction, and comprehensive evaluation â€” all locally executable.

It enables users to:

ğŸ“„ Upload multi-page PDFs (text, tables, and images)
ğŸ¤– Ask questions and get contextually grounded answers
ğŸ§© Generate multiple-choice questions (MCQs) dynamically
ğŸ” Perform reranked retrieval using a cross-encoder
ğŸ“Š Evaluate locally with ROUGE, BERTScore, and semantic metrics
ğŸ§  Monitor memory footprint and runtime statistics

## ğŸ§© Features

## ğŸ“ 1. PDF Processing & Chunking

Handles text, tables, and embedded images

Extracts structured information efficiently

Uses intelligent chunking for scalable retrieval

## ğŸ” 2. Smart Retrieval with Reranker

Employs cross-encoder re-ranking for better relevance

Integrates with FAISS-based vector retrieval

smart_retrieve() ensures precision-driven document chunks

## ğŸ’¬ 3. Question Answering

Uses LLM-backed answer generation

Ensures factual grounding using top reranked context chunks

## ğŸ¯ 4. MCQ Generation

Auto-generates MCQs from processed content

Configurable number of questions (e.g., 5, 10, 15)

Suitable for educational and comprehension tasks

## ğŸ§® 5. Evaluation Suite

Compare system answers with reference answers

Compute:

ROUGE-1, ROUGE-2, ROUGE-L

BERTScore

Semantic Similarity (SentenceTransformer)

Context Precision / Recall / Faithfulness / Correctness

Integrated through eval.py and ragadeep.py

## ğŸ§  6. Memory & Performance Profiling

Tracks:

Memory usage via psutil, pympler, tracemalloc
Processing time per document
Provides runtime statistics for optimization

## ğŸ§° Tech Stack

| Layer              | Tools / Libraries                                            |
| ------------------ | ------------------------------------------------------------ |
| **Frontend**       | Streamlit                                                    |
| **Core NLP / LLM** | OpenAI / Gemini / HuggingFace Transformers                   |
| **Retrieval**      | LangChain + FAISS                                            |
| **Reranking**      | Cross-Encoder (e.g., `cross-encoder/ms-marco-MiniLM-L-6-v2`) |
| **Evaluation**     | ROUGE, BERTScore, SentenceTransformer                        |
| **Visualization**  | Matplotlib, Plotly                                           |
| **PDF Handling**   | PyMuPDF (`fitz`), pdfplumber, ReportLab                      |
| **Performance**    | psutil, pympler, tracemalloc                                 |

## ğŸ“ File Structure

ğŸ“‚ Document-Intelligence-System/
â”‚
â”œâ”€â”€ main.py              # Main Streamlit app (PDF Q&A + MCQ + memory tracking)

â”œâ”€â”€ eval.py              # Evaluation dashboard (ROUGE, BERTScore, semantic)

â”œâ”€â”€ ragadeep.py          # Deep evaluation with readability & interpretability

â”œâ”€â”€ captiontest.py       # PDF image captioning test utility

â”œâ”€â”€ requirements.txt     # All dependencies

â””â”€â”€ README.md            # Documentation

## âš™ï¸ Installation

# Clone the repository
git clone https://github.com/ktj709/Automatic-Question-Generation-GenAI-.git
cd Automatic-Question-Generation-GenAI-

# Install dependencies
pip install -r requirements.txt

## â–¶ï¸ Running the Project
ğŸ§  Main Streamlit App

streamlit run main.py

â†’ Upload PDFs â†’ Ask questions â†’ Generate MCQs â†’ View memory usage

ğŸ“Š Evaluation App

streamlit run eval.py

â†’ Compare generated answers vs. references using ROUGE and BERTScore

ğŸ§® Deep Evaluation Suite

streamlit run ragadeep.py

â†’ Includes advanced metrics like readability, correctness, and coherence

ğŸ–¼ï¸ Caption Test

python captiontest.py

â†’ Creates a sample PDF with image captions to verify image extraction





